common:
  paths: {data: data/, results: results/}
  seed: 666
  model: 'llama'
  hf_token: 'your_huggingface_token_here'
  device: 'cuda'

tlens: 
  batch_extractor: 1024

probe:
  probe_type: 'logistic_regression'
  direction_type: 'mmp'
  lr: 1e-3
  epochs: 150
  batch_size: -1
  C: 1e6
  max_iter: 500
  test_size: 0.1
  weight_decay: 0.0
  dropout: 0.0
  var_normalize: True
  verbose: True
  control: False
  test_size: 0.2
  with_std: True
  seed: 666

coherence:
  estimator: 'logistic_regression'